import os
import glob
import numpy as np
from sklearn.metrics import jaccard_score
from PIL import Image
from anomalib.data import Folder
from anomalib.models import Patchcore
from anomalib.engine import Engine
from anomalib.post_processing import PostProcessor
from pytorch_lightning.loggers import TensorBoardLogger

# â”€â”€â”€ IoU hesaplayan fonksiyon â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def compute_iou(pred_mask: np.ndarray, gt_mask: np.ndarray, threshold: float = 0.5) -> float:
    pred_flat = (pred_mask > threshold).astype(np.uint8).flatten()
    gt_flat   = (gt_mask   > 0).astype(np.uint8).flatten()
    if gt_flat.sum() == 0 and pred_flat.sum() == 0:
        return 1.0
    if gt_flat.sum() == 0 or pred_flat.sum() == 0:
        return 0.0
    return jaccard_score(gt_flat, pred_flat, zero_division=1)

# â”€â”€â”€ 1. PostProcessor ve Model (Callback olmadan) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
post_processor = PostProcessor(
    image_sensitivity=0.5,
    pixel_sensitivity=0.5
)

# PatchCore modelini yapÄ±landÄ±rÄ±yoruz
model = Patchcore(
    backbone="wide_resnet50_2",  # ResNet backbone (Default: "wide_resnet50_2")
    layers=["layer2", "layer3"],  # Feature extraction layers (Default: ["layer2", "layer3"])
    #coreset_sampling_ratio=0.1,   # Coreset sampling iÃ§in oran (Default: 0.1)
    pre_trained=True,             # Pre-trained aÄŸÄ±rlÄ±klarÄ± kullanma (Default: True)
    post_processor=False,         # Callback olmadan post-processing (Default: True)
    evaluator=True,               # Evaluator callback (Default: True)
    visualizer=True,              # Visualizer callback (Default: True)
    num_neighbors=9,              # Nearest neighbors sayÄ±sÄ± (Default: 9)
)

# Modelin post-processor'Ä±nÄ± sadece forward kÄ±smÄ±na ekliyoruz
model.post_processor = post_processor

logger = TensorBoardLogger("tb_logs", name="patchcore")

# â”€â”€â”€ 2. DataModule â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
datamodule = Folder(
    name="wood_dataset",  # Dataset adÄ± (Default: None)
    root="/content/drive/MyDrive/datasets/wood",  # Root dizini (Default: None)
    normal_dir="train/good",  # Normal sÄ±nÄ±fÄ± iÃ§in klasÃ¶r (Default: None)
    abnormal_dir="test/defect",  # Anormal sÄ±nÄ±fÄ± iÃ§in klasÃ¶r (Default: None)
    normal_test_dir="test/good",  # Normal test verisi iÃ§in klasÃ¶r (Default: None)
    mask_dir="ground_truth/defect",  # Maskeler iÃ§in klasÃ¶r (Default: None)
    
    train_batch_size=16,  # EÄŸitim batch boyutu (Default: 32)
    eval_batch_size=16,   # Validation/test batch boyutu (Default: 32)
    num_workers=8,        # Data loading iÃ§in worker sayÄ±sÄ± (Default: 8)
    
    # PatchCore ile iliÅŸkili parametreler (varsayÄ±lan deÄŸerlerle):
    #coreset_sampling_ratio=0.1,  # Coreset sampling oranÄ± (Default: 0.1)
    #num_neighbors=9,             # Nearest neighbors sayÄ±sÄ± (Default: 9)
    #pre_trained=True,            # Pre-trained aÄŸÄ±rlÄ±klarÄ± kullanma (Default: True)
    
    normal_split_ratio=0.2,      # Normal gÃ¶rÃ¼ntÃ¼lerin validation/test olarak ayrÄ±lma oranÄ± (Default: 0.2)
    test_split_mode="from_dir",  # Test setinin hangi ÅŸekilde alÄ±nacaÄŸÄ± (Default: "from_dir")
    test_split_ratio=0.2,        # Test setinin train veri setinden ayrÄ±lacak oranÄ± (Default: 0.2)
    val_split_mode="same_as_test",  # Validation veri seti iÃ§in test veri setinden ayrÄ±lacak oran (Default: "same_as_test")
    val_split_ratio=0.5,        # Validation setinin train veri setinden ayrÄ±lacak oranÄ± (Default: 0.5)
    
    # Data augmentation parametreleri (VarsayÄ±lan olarak None):
    train_augmentations=None,  # EÄŸitim iÃ§in augmentasyon (Default: None)
    val_augmentations=None,    # Validation iÃ§in augmentasyon (Default: None)
    test_augmentations=None,   # Test iÃ§in augmentasyon (Default: None)
    augmentations=None,        # Genel augmentasyonlar (Default: None)
    
    seed=None,  # Random seed (Default: None)
)

# â”€â”€â”€ 3. Engine (Trainer Wrapper) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
engine = Engine(
    # The path where the checkpoints will be saved. (Default: "results")
    default_root_dir="/content/anomaly_detect/models",  

    # The number of epochs to train for. (Default: 1)
    max_epochs=1,  # The maximum number of epochs for training
    
    # The minimum number of epochs to train. (Default: 1)
    #min_epochs=5,  # The minimum number of epochs for training
    
    # Accelerator used for training, e.g., "auto", "cpu", "gpu". (Default: "auto")
    accelerator="auto",  # Automatically detects GPU or CPU
    
    # The number of devices (GPUs) to use for training. (Default: 1)
    devices=1,  # Use 1 GPU for training
    
    # Whether to use automatic mixed precision (AMP). (Default: False)
    precision=16,  # 16-bit precision for training
    
    # Whether to use gradient checkpointing to save memory. (Default: False)
    #gradient_checkpointing=False,  # Disabling gradient checkpointing
    
    # The number of workers to use for data loading. (Default: 8)
    #num_workers=8,  # Number of workers for loading data
    
    # A logger to log training information. (Default: None)
    logger=logger,  # TensorBoard Logger for monitoring the training
    
    # Callbacks are added to the model for events like early stopping, model checkpoints, etc. (Default: None)
    callbacks=None,  # No callbacks, but you can add early stopping or checkpoints
    
    # Path to the model weights checkpoint file. (Default: None)
    #ckpt_path="/media/taha/5E42B89C42B87A7B/LINUX/models/eff_ckpt",  # Will not load any pre-trained weights since it's not provided
    
    # Optional argument for saving checkpoints at specified intervals
    enable_checkpointing=True,  # Enable saving checkpoints during training
    
    # Validation interval in terms of epochs. (Default: 1)
    check_val_every_n_epoch=1,  # Validation check after every epoch
    
    # The validation step frequency. (Default: 1)
    val_check_interval=1.0,  # Perform validation after every epoch
    
    # Training progress bar visualization. (Default: True)
    enable_progress_bar=True,  # Enable progress bar
    
    # Whether to use automatic learning rate scheduling. (Default: True)
    #auto_lr_find=True  # Automatically find optimal learning rate
    
)

# â”€â”€â”€ 4. EÄŸitim & Test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
engine.fit(model=model, datamodule=datamodule)
engine.test(model=model, datamodule=datamodule)

# â”€â”€â”€ 5. Predict & IoU Hesaplama â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
results = engine.predict(model=model, datamodule=datamodule)
print(f"\nToplam tahmin edilen gÃ¶rÃ¼ntÃ¼ sayÄ±sÄ±: {len(results)}")

ious = []
skipped_no_mask = 0
skipped_good = 0
skipped_shape = 0

for result in results:
    img_path = result.image_path[0]

    # Normal (good) test gÃ¶rÃ¼ntÃ¼lerini atla
    if "/test/good/" in img_path:
        skipped_good += 1
        continue

    # Tahmin maskesini al (zaten normalize + threshold edilmiÅŸ)
    pred_mask = result.anomaly_map[0].detach().cpu().numpy()

    # Dosya adÄ± ve GT mask adÄ±
    filename = os.path.basename(img_path)
    mask_filename = filename.replace(".jpg", "_mask.jpg")

    # GT mask yolunu tÃ¼m alt klasÃ¶rlerde ara
    mask_pattern = os.path.join(
        "/content/drive/MyDrive/datasets/wood",
        "ground_truth/defect", "**", mask_filename
    )
    matches = glob.glob(mask_pattern, recursive=True)

    if not matches:
        print(f"GT mask bulunamadÄ±: {mask_pattern}")
        skipped_no_mask += 1
        continue

    gt_path = matches[0]

    # PIL ile grayscale olarak oku (HÃ—W)
    gt_mask = np.array(Image.open(gt_path).convert("L"))

    # Boyut uyumsuzluÄŸu kontrolÃ¼
    if pred_mask.shape != gt_mask.shape:
        print(f"Boyut uyumsuzluÄŸu: pred {pred_mask.shape}, gt {gt_mask.shape}")
        skipped_shape += 1
        continue

    # IoU hesapla ve listeye ekle
    iou = compute_iou(pred_mask, gt_mask, threshold=0.5)
    print(f"{filename} â†’ IoU: {iou:.4f}")
    ious.append(iou)

# â”€â”€â”€ 6. SonuÃ§ Ã–zeti â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\nğŸ§¾ Ã–zet:")
print(f"  Toplam tahmin edilen Ã¶rnek      : {len(results)}")
print(f"  Atlanan good sÄ±nÄ±fÄ± gÃ¶rÃ¼ntÃ¼ler  : {skipped_good}")
print(f"  Eksik GT mask nedeniyle atlanan : {skipped_no_mask}")
print(f"  Boyut uyumsuzluÄŸu nedeniyle atla: {skipped_shape}")
print(f"  IoU hesaplanan gÃ¶rÃ¼ntÃ¼ sayÄ±sÄ±   : {len(ious)}")

if ious:
    print(f"\nâœ… Ortalama IoU: {np.mean(ious):.4f}")
else:
    print("\nâš ï¸  HiÃ§bir defect gÃ¶rÃ¼ntÃ¼sÃ¼ iÃ§in IoU hesaplanamadÄ±!")

